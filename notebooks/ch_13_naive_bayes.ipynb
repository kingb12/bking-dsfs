{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes is a classifier that uses Bayes theorem to estimate the probablity for P(some class | some features) using P(some features | some class) and Bayes Theorem.\n",
    "\n",
    "Bayes Theorem (e.g. A = 'message is spam', B = 'message contains the word 'exclusive'')\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has a simple proof using the definition of 'and' in probability theory, as well as conditional probability: start from P(AB) and derive to conditional form for each, then isolate conditionals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$1. P(A \\cap B) = P(A|B)P(B)$$\n",
    "\n",
    "$$2. P(A \\cap B) = P(B|A)P(A)$$\n",
    "\n",
    "$$3. P(A|B)P(B) = P (B | A)P(A)$$\n",
    "\n",
    "$$4. P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Naive Bayes spam-filter\n",
    "\n",
    "In our spam filter construction, lets assume that we want to model  P(spam|email content) as P(spam |bag-of-words(email)). We can use Bayes Theorem to instead model this like so:\n",
    "\n",
    "$$P(spam|BOW(email)) = \\frac{P(BOW(email) | spam)P(spam)}{P(BOW(email))}$$\n",
    "\n",
    "\n",
    "This is nearly impossible to estmate from data frequencies as-is, but if we assume that each word contributes to P(spam) individually (obviously wrong but works well enough in practice), then we can model the probabilty of each word in the bag independently:\n",
    "\n",
    "$$P(BOW(email) | spam) = \\prod_{w_i \\in BOW(email)} P(w_i | spam)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$P(BOW(email)) = \\prod_{w_i \\in BOW(email)} P(w_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is the correct definition, in practice we want to smooth for the fact that whatever labelled dataset we have doesn't represent the domain completely. For instance, if 'bitcoin' only occurs once in the dataset and that message is *not* spam, then we will estimate a 0% chance of spam in the case the message has the word bitcoin:\n",
    "\n",
    "$$P(spam|BOW(email)) = \\frac{P(BOW(email) | spam)P(spam)}{P(BOW(email))}$$\n",
    "\n",
    "$$P(spam|BOW(email)) = \\frac{P(spam)\\prod_{w_i \\in BOW(email)} P(w_i | spam)}{P(BOW(email))}$$\n",
    "\n",
    "$$P(spam|BOW(email)) = \\frac{P(spam)(x_0 * 0 ...  x_V)}{P(BOW(email))} = \\textbf{0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To account for this, we begin with a uniform 'pseudocount' for all possible words: effectively assuming we've seen each word at least k times per class already**\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class called NaiveBayesClassifier that has a predict and train method\n",
    "# class \n",
    "from typing import Tuple, Set, NamedTuple, Iterable, Dict, List\n",
    "from collections import defaultdict, Counter\n",
    "import re, math, os, glob, random, sys\n",
    "import tqdm\n",
    "\n",
    "from io import BytesIO\n",
    "import requests, tarfile\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from machine_learning import split_data, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(email: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    tokenize the email string into a case-insensitive bag-of-words\n",
    "    \"\"\"\n",
    "    email = email.lower()\n",
    "    tokens = re.findall('[a-z0-9]+', email)\n",
    "    return set(tokens)\n",
    "assert tokenize('Data Science is science') == {'data', 'is', 'science'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(NamedTuple):\n",
    "    \"\"\"\n",
    "    Class representing an email message and  whether it is spam or not\n",
    "    \"\"\"\n",
    "    email: str  # original email content\n",
    "    spam: bool  # true if spam, false if ham\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    \"\"\"\n",
    "    A Naive Bayes Classifier operating on Messages, determining if it is spam or ham\n",
    "    \"\"\"\n",
    "    k: int  # pseudocount: just do the math with this value so we can swap k params without re-training\n",
    "    spam_count: int  # the number of messages in our training set that were spam\n",
    "    ham_count: int  # the number of messages in our training set that were NOT spam\n",
    "    spam_token_counts: Dict[str, int]  # per-token frequencies when an email is spam\n",
    "    ham_token_counts: Dict[str, int]  # per-token frequencies when an email is NOT spam\n",
    "    all_tokens: Set[str]  # all the tokens in the dataset\n",
    "    p_spam_prior: float  # real-world expected probability of spam\n",
    "    \n",
    "    def __init__(self, k:float = 1, p_spam_prior:float = None):\n",
    "        self.k = k\n",
    "        self.spam_count = 0\n",
    "        self.ham_count = 0\n",
    "        self.spam_token_counts = defaultdict(int)\n",
    "        self.ham_token_counts = defaultdict(int)\n",
    "        self.all_tokens = set()\n",
    "        self.p_spam_prior = p_spam_prior\n",
    "    \n",
    "    def train(self, emails: Iterable[Message]) -> None:\n",
    "        \"\"\"\n",
    "        Train our classifier on the emails\n",
    "        \"\"\"\n",
    "        for email in tqdm.tqdm(emails):\n",
    "            # account for P(spam) and P(!spam)\n",
    "            if email.spam:\n",
    "                self.spam_count += 1                \n",
    "            else:\n",
    "                self.ham_count += 1\n",
    "            # now modify per-class token frequencies\n",
    "            tokens: Set[str] = tokenize(email.email)\n",
    "            for token in tokens:\n",
    "                self.all_tokens.add(token)\n",
    "                if email.spam:\n",
    "                    self.spam_token_counts[token] += 1\n",
    "                else:\n",
    "                    self.ham_token_counts[token] += 1\n",
    "    \n",
    "    def _p_token(self, token: str, given_spam: bool) -> float:\n",
    "        \"\"\"\n",
    "        Given a token, return the probability of its occurence in a spam (given_spam=True) or\n",
    "        ham (given_spam=False) message\n",
    "        \"\"\"\n",
    "        # get the count of our token in the spam or ham context, as well as the total count of that context\n",
    "        token_count: int = self.spam_token_counts[token] if given_spam else self.ham_token_counts[token]\n",
    "        message_count: int = self.spam_count if given_spam else self.ham_count\n",
    "        # compute p as smoothed by k\n",
    "        p_token: float = float(token_count + self.k) / ((2 * self.k) + message_count)\n",
    "        return p_token\n",
    "            \n",
    "    def predict(self, email:str) -> bool:\n",
    "        \"\"\"\n",
    "        Predict whether a particular email is spam (true) or ham (false)\n",
    "        \"\"\"\n",
    "        # first, tokenize it\n",
    "        email_tokens: Set[str] = tokenize(email)\n",
    "        # We'll be computing conditional probs of each token (or its absence) using a log-sum to avoid underflow\n",
    "        log_p_sum_given_spam, log_p_sum_given_ham = 0, 0\n",
    "        # for each POSSIBLE token, appropriately modify these sums\n",
    "        for token in self.all_tokens:\n",
    "            # how often do we expect to see this token in a spam or ham message?\n",
    "            p_token_given_spam, p_token_given_ham  = self._p_token(token, True), self._p_token(token, False)\n",
    "            if token in email_tokens:\n",
    "                # this token is present, modify our conditional prob for spam/ham to note its presence\n",
    "                log_p_sum_given_spam += math.log(p_token_given_spam)\n",
    "                log_p_sum_given_ham += math.log(p_token_given_ham)\n",
    "            else:\n",
    "                 # this token is absent, modify our conditional prob for spam/ham to note its absence\n",
    "                log_p_sum_given_spam += math.log(1 - p_token_given_spam)\n",
    "                log_p_sum_given_ham += math.log(1 - p_token_given_ham)\n",
    "        # now compute our conditional probabilities P(BOW(email) | spam) and P(BOW(email) | !spam)\n",
    "        p_tokens_given_spam, p_tokens_given_ham = math.exp(log_p_sum_given_spam), math.exp(log_p_sum_given_ham)\n",
    "        # using Bayes Theorem, our prediction for P(spam | BOW(email)) is:\n",
    "        p_spam: float = float(self.spam_count / (self.spam_count + self.ham_count))\n",
    "        p_ham: float = 1 - p_spam\n",
    "        # if no class_balance_prior, use real frequencies in training data\n",
    "        class_ratio: float = p_ham / p_spam if self.p_spam_prior is None else (1 - self.p_spam_prior) / self.p_spam_prior\n",
    "        return p_tokens_given_spam / (p_tokens_given_spam + (p_tokens_given_ham * class_ratio))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivation of the `predict` return quantity:\n",
    "\n",
    "$$P(spam|BOW(email)) = \n",
    "\\frac{P(BOW(email) | spam)P(spam)}\n",
    "{P(BOW(email)|spam)P(spam) + P(BOW(email) | \\neg{spam})P(\\neg{spam})}$$\n",
    "\n",
    "Becomes:\n",
    "\n",
    "$$P(spam|BOW(email)) = \n",
    "\\frac{P(BOW(email) | spam)}{P(BOW(email)|spam) + P(BOW(email) | \\neg{spam}) * \\frac{P(\\neg{spam})}{P(spam)})}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 9868.95it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 17747.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Some unit tests!\n",
    "messages = [Message(\"spam rules\", spam=True), Message(\"Ham rules\", spam=False), Message(\"hello ham\", spam=False)]\n",
    "model = NaiveBayesClassifier(k=0.5)\n",
    "model.train(messages)\n",
    "\n",
    "# verify correct counting after train:\n",
    "assert model.spam_count == 1\n",
    "assert model.ham_count == 2\n",
    "assert model.spam_token_counts == {'spam': 1, 'rules': 1}\n",
    "assert model.ham_token_counts == {'ham': 2, 'rules': 1, 'hello': 1}\n",
    "assert model.all_tokens == {'spam', 'ham', 'rules', 'hello'}\n",
    "\n",
    "# verify correct predictions (the hard way)\n",
    "text = 'hello spam'\n",
    "probs_if_spam = [\n",
    "    (1 + .5) / (1 + .5*2), # spam (present)\n",
    "    1 - ((0 + .5) / (1 + .5*2)), # ham (absent)\n",
    "    1 - ((1 + .5) / (1 + .5*2)), # rules (absent)\n",
    "    (0 + .5) / (1 + .5*2), # hello (present)\n",
    "]\n",
    "\n",
    "probs_if_ham = [\n",
    "    (0 + .5) / (2 + .5*2), # spam\n",
    "    1 - ((2 + .5) / (2 + .5*2)), # ham\n",
    "    1 - ((1 + .5) / (2 + .5*2)), # rules\n",
    "    (1 + .5) / (2 + .5*2), # hello\n",
    "]\n",
    "\n",
    "p_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam))\n",
    "p_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham))\n",
    "assert abs(model.predict(text) - p_if_spam / (p_if_spam + (p_if_ham * (2.)))) < 10**-7\n",
    "\n",
    "# now verify result (like book) when we assume spam and ham are balanced classes in the real world\n",
    "model = NaiveBayesClassifier(k=0.5, p_spam_prior=0.5)\n",
    "model.train(messages)\n",
    "assert abs(model.predict(text) - p_if_spam / (p_if_spam + p_if_ham)) < 10**-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our model on SpamAssasin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://spamassassin.apache.org/old/publiccorpus/'\n",
    "FILES = ['20021010_easy_ham.tar.bz2','20021010_hard_ham.tar.bz2', '20021010_spam.tar.bz2']\n",
    "\n",
    "OUTPUT_DIR = '/Users/bking/bking-dsfs/spam_data'\n",
    "if not os.path.isfile(OUTPUT_DIR) or len(os.listdir(OUTPUT_DIR)) == 0:\n",
    "    for fname in FILES:\n",
    "        content = requests.get(BASE_URL + fname).content\n",
    "        fin = BytesIO(content)\n",
    "        with tarfile.open(fileobj=fin, mode='r:bz2') as tf:\n",
    "            tf.extractall(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = OUTPUT_DIR + '/*/*'\n",
    "data: List[Message] = []\n",
    "\n",
    "for fname in glob.glob(path):\n",
    "    is_spam = 'ham' not in fname\n",
    "    \n",
    "    with open(fname, errors='ignore') as ef:\n",
    "        for line in ef:\n",
    "            if line.startswith('Subject:'):\n",
    "                subject = line.lstrip('Subject: ')\n",
    "                data.append(Message(subject, is_spam))\n",
    "                break  # done with this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2475/2475 [00:00<00:00, 101918.44it/s]\n",
      "100%|██████████| 2475/2475 [00:00<00:00, 132821.15it/s]\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "train_msgs, test_msgs = split_data(data, .75)\n",
    "\n",
    "model_book = NaiveBayesClassifier(p_spam_prior=0.5)\n",
    "model_me = NaiveBayesClassifier(p_spam_prior=None)\n",
    "model_book.train(train_msgs)\n",
    "model_me.train(train_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_book = [(msg, model_book.predict(msg.email)) for msg in test_msgs]\n",
    "predictions_me = [(msg, model_me.predict(msg.email)) for msg in test_msgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_book = Counter((msg.spam, p_spam > 0.5) for msg, p_spam in predictions_book)\n",
    "confusion_matrix_me = Counter((msg.spam, p_spam > 0.5) for msg, p_spam in predictions_me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(False, False): 679, (True, False): 81, (True, True): 58, (False, True): 7})\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(False, False): 683, (True, False): 102, (True, True): 37, (False, True): 3})\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix_me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function f1_score in module machine_learning:\n",
      "\n",
      "f1_score(true_positives: int, false_positives: int, false_negatives: int, true_negatives: int) -> float\n",
      "    compute the F1 score from the confusion matrix\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books F1 Score:  0.5686274509803922\n",
      "My F1 Score:  0.41340782122905034\n"
     ]
    }
   ],
   "source": [
    "cmb, cme = confusion_matrix_book, confusion_matrix_me\n",
    "print('Books F1 Score: ', f1_score(cmb[(True, True)], cmb[(True, False)], cmb[(False, True)], cmb[(False, False)]))\n",
    "print('My F1 Score: ', f1_score(cme[(True, True)], cme[(True, False)], cme[(False, True)], cme[(False, False)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So do what the book says I guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsfsvenv",
   "language": "python",
   "name": "dsfsvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
