{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "A decision tree is basically a game of 20 (or N) questions, used against the features of a dataset to build tree and determine a class.\n",
    "\n",
    "**Strengths:** \n",
    "- works well with both categorical and real-valued feature data, and can also produce categorical or real-valued output.\n",
    "- very easy to interpret, relative to other methods\n",
    "\n",
    "**Weaknesses:**\n",
    "- overfitting a significant risk\n",
    "- computationally hard to find an optimal tree. There are strategies for finding 'good' ones, but for datasets with complex inter-feature relationships, this can be very difficult to get a performant one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "When playing 20Q, the question-asker aims to gain as much information as possible about what the right answer(s) might be, from a simple yes/no. (e.g. when guessing an animal species, its not efficient to ask \"is it a tiger?\" as the first question: very little information is gained in terms of classes that can be ruled-out.\n",
    "\n",
    "We quantify this using a construct called *entropy* (the basis for cross-entropy loss), which quantifies the uncertainty in a dataset.\n",
    "\n",
    "Imagine a data-point (animal) can belong to one of N classes (species). Given a distribution of classes and their probabilities, *entropy* tells us how much we could infer about the data-point given the dataset (or relevant, pruned subset as determined by a decision tree).\n",
    "\n",
    "Given $N$ classes and a set of data $S$, the entropy of the data-set $H(S)$ is given by:\n",
    "$$H(S) = -\\sum_{i=1}^{n}{p_i\\log_2{p_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are probably more interesting details to fill in, but the interior term has the property that each is non-positive (making the overall negative sum non-negative), and entropy is minimized as $p_i$ nears *either* 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Awful hack to import past chapter modules\n",
    "import os, sys, json\n",
    "sys.path.insert(0, \"../\")\n",
    "import csv\n",
    "from linalg import Vector, distance\n",
    "from multiple_regression import bootstrap_sample\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import List, Any, NamedTuple, TypeVar, Dict, Tuple, Optional, Union\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graphing the entropy term for just one class\n",
    "xs = [x / 100. for x in range(0, 100)]\n",
    "ys = [-1 * p * (math.log(p, 2) if p > 0 else 0) for p in xs]\n",
    "plt.plot(xs, ys)\n",
    "plt.title(\"$p_i\\log_2{p_i}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interesting:** Why is this not maximized at 0.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(class_probabilities: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Given a list of the class sample probabilities for ALL known classes, computes the entropy of the overall dataset\n",
    "    \"\"\"\n",
    "    return -1 * sum([p * (math.log(p, 2) if p > 0 else 0) for p in class_probabilities])\n",
    "\n",
    "assert(entropy([0, 0]) == 0)\n",
    "assert(entropy([]) == 0)\n",
    "assert(entropy([1, 1]) == 0)\n",
    "assert(entropy([.5, .5]) == 1)\n",
    "assert(.81 < entropy([.25, .75]) < .82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_probabilities(labels: List[Any]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Given a list of class labels in a data-set (raw list of all observed labels), compute class probabilities for each\n",
    "    \"\"\"\n",
    "    counts: Counter = Counter(labels)\n",
    "    total = len(labels)\n",
    "    return [float(count) / total for count in counts.values()]\n",
    "\n",
    "# order and connection to what the labels are is not preserved\n",
    "assert(set(class_probabilities([1, 2, 3, 4, 1, 2, 2, 2])) == {.5, .25, .125, .125})\n",
    "def dataset_entropy(labels: List[Any]) -> float:\n",
    "    \"\"\"\n",
    "    composes class probabilities and entropy to get the entropy of the whole dataset\n",
    "    \"\"\"\n",
    "    return entropy(class_probabilities(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Entropy\n",
    "\n",
    "Above, we have the tools to compute the entropy of an entire dataset. In building a decision tree, our goal will be to apply this to the partitions of a decision tree as defined by a particular question node. To do this, we'll compute a partition entropy: provided with a partition of a dataset (a list of lists), we'll compute the overall entropy of this partitioning as the entropy of each partition multiplied by the proportion of labels in that partition ($q_1H(S_1)1 + ... + q_mH(S_m) $):\n",
    "\n",
    "$$ H = \\sum_{i=1}^m q_i * H(S_i) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy(partitions: List[List[Any]]) -> float:\n",
    "    \"\"\"\n",
    "    Given a partitioning of a dataset, compute the entropy of that partitioning as \n",
    "    the sum of the entropy of each partition multiplied by the proportion of labels it contains\n",
    "    \"\"\"\n",
    "    total_count: int = sum(len(part) for part in partitions)\n",
    "    return sum(dataset_entropy(part) * len(part) / total_count for part in partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over-fitting as a pitfall of minimizing entropy\n",
    "\n",
    "Overall, we want to build a decision tree whicvh minimizes entropy in the leaves it generates. In fact, one of the most common algorithms for building an 'optimal' decision tree greedily chooses nodes that maxmize information gain, or the difference in entropy between the current partitioning and a future one. \n",
    "\n",
    "This leads to over-fitting, as a leaf node with only one training value has zero entropy. For features with a large variety of values this optimizes for a partitioning that creates few-example or single-example leaf nodes. If these differences don't generalize though, then the model is overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Decision Tree\n",
    "\n",
    "Putting this together for an algorithm which produces a decision tree, we want to do the following:\n",
    "\n",
    "**Inputs:** A data set consisting of a list of discretely featured points and a discrete class label (this generalizes to real value features and real value labels, though).\n",
    "\n",
    "**Outputs:** A tree which can be used to predict a class label from the features of the point\n",
    "\n",
    "##### Algorithm\n",
    "```\n",
    "make_decision_tree(data):\n",
    "    if (all data have the same label) \n",
    "        create a leaf node which predicts this label\n",
    "        DONE\n",
    "    if (there are no more possible features with which to differentiate the dataset)\n",
    "        create a leaf node predicting the most common label\n",
    "        DONE\n",
    "    for EACH attribute:\n",
    "        parts = partition the data on that attribute\n",
    "        entropy = compute entropy of that partitioning\n",
    "    create a decision node using the partitioning with the least entropy\n",
    "    for EACH part:\n",
    "        compute a sub-tree for each partitioning and attach it to this decision node\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My very cool data\n",
    "class Animal(NamedTuple):\n",
    "    \"\"\"\n",
    "    Everything important to know about animals\n",
    "    \"\"\"\n",
    "    legs: int\n",
    "    swims: bool\n",
    "    claws: bool\n",
    "    bites: bool\n",
    "    colonizes: bool\n",
    "    name: str\n",
    "\n",
    "        \n",
    "data = [\n",
    "    Animal(2, True, False, True, True, 'human'),\n",
    "    Animal(6, False, False, False, True, 'ant'),\n",
    "    Animal(4, False, True, True, False, 'cat'),    \n",
    "    Animal(4, True, True, True, False, 'dog'),    \n",
    "    Animal(4, True, True, True, False, 'bear'),    \n",
    "    Animal(4, False, False, True, False, 'horse'),    \n",
    "    Animal(8, True, False, False, False, 'octopus'),    \n",
    "    Animal(2, True, False, False, False, 'fish'),    \n",
    "    Animal(6, True, False, False, False, 'squid'),    \n",
    "    Animal(6, False, False, False, True, 'bee'),    \n",
    "    Animal(6, False, False, False, False, 'butterfly'),    \n",
    "    Animal(2, False, True, True, False, 'eagle'),    \n",
    "    Animal(2, False, True, True, False, 'hawk'),    \n",
    "    Animal(2, False, True, True, False, 'robin'),    \n",
    "    Animal(2, False, True, True, False, 'crow'),    \n",
    "]\n",
    "\n",
    "# Joel's boring dubious data\n",
    "class Candidate(NamedTuple):\n",
    "    level: str\n",
    "    lang: str\n",
    "    tweets: bool\n",
    "    phd: bool\n",
    "    did_well: Optional[bool] = None  # allow unlabeled data\n",
    "\n",
    "                  #  level     lang     tweets  phd  did_well\n",
    "inputs = [Candidate('Senior', 'Java',   False, False, False),\n",
    "          Candidate('Senior', 'Java',   False, True,  False),\n",
    "          Candidate('Mid',    'Python', False, False, True),\n",
    "          Candidate('Junior', 'Python', False, False, True),\n",
    "          Candidate('Junior', 'R',      True,  False, True),\n",
    "          Candidate('Junior', 'R',      True,  True,  False),\n",
    "          Candidate('Mid',    'R',      True,  True,  True),\n",
    "          Candidate('Senior', 'Python', False, False, False),\n",
    "          Candidate('Senior', 'R',      True,  False, True),\n",
    "          Candidate('Junior', 'Python', True,  False, True),\n",
    "          Candidate('Senior', 'Python', True,  True,  True),\n",
    "          Candidate('Mid',    'Python', False, True,  True),\n",
    "          Candidate('Mid',    'Java',   True,  False, True),\n",
    "          Candidate('Junior', 'Python', False, True,  False)\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar('T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_by(points: List[T], attribute: str) -> Dict[Any, List[T]]:\n",
    "    \"\"\"\n",
    "    Given a list of points, partition them by the attribute retrievable at <attribute>\n",
    "    \"\"\"   \n",
    "    parts: Dict[Any, List[T]] = defaultdict(list)\n",
    "    for p in points:\n",
    "        key = getattr(p, attribute)\n",
    "        parts[key].append(p)\n",
    "    return parts\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy_by(points: List[T], attribute: str, label_attribute: str) -> Tuple[Dict[Any, List[T]], float]:\n",
    "    \"\"\"\n",
    "    Given a list of points, partition them by the attribute retrievable at <attribute> and compute the entropy of \n",
    "    each partition with respect to the attribute retrievable at <label_attribute>\n",
    "    \"\"\"\n",
    "    parts: Dict[Any, List[T]] = partition_by(points, attribute)\n",
    "        \n",
    "    # partition_entropy requires us to process this data into just label buckets\n",
    "    just_labels = []\n",
    "    for key, vals in parts.items():\n",
    "        just_labels.append([getattr(v, label_attribute) for v in vals])\n",
    "    return parts, partition_entropy(just_labels)\n",
    "\n",
    "assert 0.69 < partition_entropy_by(inputs, 'level', 'did_well')[1] < 0.70\n",
    "assert 0.86 < partition_entropy_by(inputs, 'lang', 'did_well')[1] < 0.87\n",
    "assert 0.78 < partition_entropy_by(inputs, 'tweets', 'did_well')[1] < 0.79\n",
    "assert 0.89 < partition_entropy_by(inputs, 'phd', 'did_well')[1] < 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tree will either be a leaf note, or a split\n",
    "DecisionNode = Union['Leaf', 'Split']\n",
    "class Leaf(NamedTuple): \n",
    "    value: Any\n",
    "        \n",
    "class Split(NamedTuple):\n",
    "    attribute: str\n",
    "    subtrees: Dict[Any, DecisionNode]\n",
    "    default_value: Any = None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(node: DecisionNode, input: Any):\n",
    "    \"\"\"\n",
    "    Given a decision tree node and input to classify with that decision tree, \n",
    "    determine the appropriate label for the input\n",
    "    \"\"\"\n",
    "    while(True):\n",
    "        if isinstance(node, Leaf):\n",
    "            return node.value\n",
    "        else:\n",
    "            # split: follow the right branch\n",
    "            value = getattr(input, node.attribute)\n",
    "            if value not in node.subtrees:\n",
    "                return node.default_value\n",
    "            # continue\n",
    "            node = node.subtrees[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_id3(data: List[Any], split_attributes: List[str], target_attribute: str) -> DecisionNode:\n",
    "    \"\"\"\n",
    "    Given a dataset, a set of attributes belonging to the items in that dataset, and a target attribute, return\n",
    "    the root node of a deicision tree which splits upon split_attributes, seeking to predict an inputs target\n",
    "    attribute\n",
    "    \"\"\"\n",
    "    counts = Counter([getattr(p, target_attribute) for p in data])\n",
    "    most_common_target_attr = counts.most_common(1)[0][0]\n",
    "    if len(counts) == 1:\n",
    "        # everything has the same target attribute, just return a node with it\n",
    "        return Leaf(most_common_target_attr)\n",
    "    elif len(split_attributes) == 0:\n",
    "        # nothing left to split on, just return the most common target attribute\n",
    "        return Leaf(most_common_target_attr)\n",
    "    parts_by_entropy: Dict[float, Tuple[str, Dict[Any, List[T]]]] = {}\n",
    "    for attr in split_attributes:\n",
    "        parts, entropy = partition_entropy_by(data, attr, target_attribute)\n",
    "        # duplicate keys not an issue: if two share the lowest entropy we split on one arbitrarily\n",
    "        parts_by_entropy[entropy] = (attr, parts)\n",
    "    # find the partitioning with lowest entropy and create a Split with it\n",
    "    attr, parts = parts_by_entropy[min(parts_by_entropy.keys())]\n",
    "    subtrees: Dict[Any, DecisionNode] = {}\n",
    "    for attr_val in parts:\n",
    "        # recurse over sub-trees with one less split attribute\n",
    "        subtrees[attr_val] = build_tree_id3(parts[attr_val], [a for a in split_attributes if a != attr], target_attribute)          \n",
    "    return Split(attr, subtrees, most_common_target_attr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " \"level\",\n",
      " {\n",
      "  \"Senior\": [\n",
      "   \"tweets\",\n",
      "   {\n",
      "    \"false\": [\n",
      "     false\n",
      "    ],\n",
      "    \"true\": [\n",
      "     true\n",
      "    ]\n",
      "   },\n",
      "   false\n",
      "  ],\n",
      "  \"Mid\": [\n",
      "   true\n",
      "  ],\n",
      "  \"Junior\": [\n",
      "   \"phd\",\n",
      "   {\n",
      "    \"false\": [\n",
      "     true\n",
      "    ],\n",
      "    \"true\": [\n",
      "     false\n",
      "    ]\n",
      "   },\n",
      "   true\n",
      "  ]\n",
      " },\n",
      " true\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# verify on Joels data\n",
    "tree = build_tree_id3(inputs, ['level', 'lang', 'tweets', 'phd'], 'did_well')\n",
    "print(json.dumps(tree, indent=1))\n",
    "assert classify(tree, Candidate('Junior', 'Java', True, False))\n",
    "assert not classify(tree, Candidate('Junior', 'Java', True, True))\n",
    "# unexpected value happens to return true\n",
    "assert classify(tree, Candidate('Intern', 'Java', True, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Not surprisingly, decision trees as constructed above tend to significantly overfit. Several options for handling this:\n",
    "\n",
    "1. Stoppage criteria [fixed depth or number of leaves (or something smarter)](https://courses.cs.washington.edu/courses/cse446/16sp/slides/week_1_2_decision_trees_Apr1.pdf)\n",
    "2. Random forest: build several decision trees using the exact algorithm above but with bootstrapped data (sub-sets). This technique actually allows accurate performance assessment on the whole dataset if done carefully (no need for a test set).\n",
    "3. Random forest 2: build several decision trees where at each split_attribute opportunity, you only consider some of the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest = List[DecisionNode]\n",
    "def random_bootstrapped_forest(data: List[Any], split_attributes: List[str], target_attribute: str, n: int = 10, sampling_size: int = 0) -> RandomForest:\n",
    "    \"\"\"\n",
    "    Train n decision trees using bootstrapped sub-samples of the input dataset\n",
    "    \"\"\"\n",
    "    forest: RandomForest = []\n",
    "    if sampling_size <= 0:\n",
    "        sampling_size = len(data)\n",
    "    for i in range(n):\n",
    "        sub_sample: List[Any] = bootstrap_sample(data, sampling_size)\n",
    "        forest.append(build_tree_id3(sub_sample, split_attributes, target_attribute))\n",
    "    return forest\n",
    "\n",
    "        \n",
    "def random_forest_vote_classify(forest: RandomForest, x: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Classify using a random forest, using votes as the response mechanism\n",
    "    \"\"\"\n",
    "    votes: List[Any] = []\n",
    "    for tree in forest:\n",
    "        votes.append(classify(tree, x))\n",
    "    return Counter(votes).most_common(1)[0][0]\n",
    "        \n",
    "        \n",
    "# verify on Joels data\n",
    "forest: RandomForest = random_bootstrapped_forest(inputs, ['level', 'lang', 'tweets', 'phd'], 'did_well', n=200, sampling_size=0)\n",
    "assert random_forest_vote_classify(forest, Candidate('Junior', 'R', True, False))\n",
    "assert not random_forest_vote_classify(forest, Candidate('Junior', 'R', True, True))\n",
    "# unexpected value happens to return true\n",
    "assert random_forest_vote_classify(forest, Candidate('Intern', 'Java', True, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsfsvenv",
   "language": "python",
   "name": "dsfsvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
