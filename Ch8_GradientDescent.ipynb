{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Optimizing functions algorithmically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linalg import Vector, dot\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(xs: Vector) -> float:\n",
    "    \"\"\"\n",
    "    Return the sum of the square of each element in xs\n",
    "    \"\"\"\n",
    "    # this is equivalent to x dot x\n",
    "    return dot(x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a function (e.g. a loss function) which reduces a vector to a meaningful float. The main idea of gradient descent is to algorithmically find the inputs that minimize this reducing function\n",
    "\n",
    "### Terms:\n",
    "- **Gradient**: The vector of partial deriviates for a vector relative to a function. E.g. if `y = sum_of_squares(xs)`, the gradirent is `dy/dxs` or `[dy/dx_0, dy/dx_1, ... dy/dx_n]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_quotient(f: Callable[[float], float], x: float, h: float) -> float:\n",
    "    return (f(x + h) - f(x)) / h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the definition of a gradient for a single variable x and function f(x). We can estimate the gradient by just choosing a very small h (e.g. 10**-6). We can also do this for partial-derivatives in a vector calculus setting for f(xs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_diff_quotient(f: Callable[[Vector], float], xs: Vector, i: int, h: float) -> float:\n",
    "    w = [x_j + (h if i == j else 0) for j, x_j in enumerate(xs)]  # single out and add h to just the ith element of xs\n",
    "    return (f(w) - f(xs)) / h  # reflects only the change we made to the ith variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[Vector], float], xs: Vector, h: float = 10**-4) -> Vector:\n",
    "    \"\"\"\n",
    "    Estimate the gradient of f with respect to xs by computing partial diff quotients element-wise\n",
    "    \"\"\"\n",
    "    # note this is expensive and why auto-grad libraries mathematically compute most derivatives\n",
    "    return [partial_diff_quotient(f, xs, i, h) for i in range(len(xs))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Gradient\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsfsvenv",
   "language": "python",
   "name": "dsfsvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
