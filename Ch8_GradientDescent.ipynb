{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Optimizing functions algorithmically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linalg import Vector, dot, distance, add, scalar_multiply, add, vector_mean\n",
    "from typing import Callable\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(xs: Vector) -> float:\n",
    "    \"\"\"\n",
    "    Return the sum of the square of each element in xs\n",
    "    \"\"\"\n",
    "    # this is equivalent to x dot x\n",
    "    return dot(x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a function (e.g. a loss function) which reduces a vector to a meaningful float. The main idea of gradient descent is to algorithmically find the inputs that minimize this reducing function\n",
    "\n",
    "### Terms:\n",
    "- **Gradient**: The vector of partial deriviates for a vector relative to a function. E.g. if `y = sum_of_squares(xs)`, the gradirent is `dy/dxs` or `[dy/dx_0, dy/dx_1, ... dy/dx_n]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_quotient(f: Callable[[float], float], x: float, h: float) -> float:\n",
    "    return (f(x + h) - f(x)) / h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the definition of a gradient for a single variable x and function f(x). We can estimate the gradient by just choosing a very small h (e.g. 10**-6). We can also do this for partial-derivatives in a vector calculus setting for f(xs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_diff_quotient(f: Callable[[Vector], float], xs: Vector, i: int, h: float) -> float:\n",
    "    w = [x_j + (h if i == j else 0) for j, x_j in enumerate(xs)]  # single out and add h to just the ith element of xs\n",
    "    return (f(w) - f(xs)) / h  # reflects only the change we made to the ith variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[Vector], float], xs: Vector, h: float = 10**-4) -> Vector:\n",
    "    \"\"\"\n",
    "    Estimate the gradient of f with respect to xs by computing partial diff quotients element-wise\n",
    "    \"\"\"\n",
    "    # note this is expensive and why auto-grad libraries mathematically compute most derivatives\n",
    "    return [partial_diff_quotient(f, xs, i, h) for i in range(len(xs))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Gradient\n",
    "\n",
    "For sum of squares, it is obvious the minimum overall is achieved by a vector of zeros. In some cases though, the minimum(s) may not be obvious, so we'll use it as an example to evaluatate our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(xs: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"\n",
    "    Moves `step_size` along the gradient of f w.r.t. xs, returning a input\n",
    "    \"\"\"\n",
    "    assert len(xs) == len(gradient)\n",
    "    update = scalar_multiply(step_size, gradient)\n",
    "    return add(xs, update)\n",
    "\n",
    "def sum_of_squares_gradient(xs: Vector) -> Vector:\n",
    "    \"\"\"\n",
    "    We know the partial-derivative for a sum of squares is just 2*`the_term`\n",
    "    \"\"\"\n",
    "    return [2*x for x in xs]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1522938445231776e-08, 1.5514131039526043e-09, 9.878463039780581e-09, 1.941546825631437e-08, -1.5841937294312695e-08, 1.110593240200334e-09, 1.524930648300702e-08, 9.542884342392084e-09, -1.373269285380657e-08, -1.2214808198697881e-08]\n"
     ]
    }
   ],
   "source": [
    "# Now lets verify with an experiment\n",
    "xs = [random.uniform(-10, 10) for i in range(10)]\n",
    "\n",
    "for i in range(1, 100000):\n",
    "    grad = sum_of_squares_gradient(xs)\n",
    "    xs = gradient_step(xs, grad, -1 * (10**-4))\n",
    "\n",
    "assert(distance(xs, np.zeros(10)) < 10**-6) # we should have gotten very close to zero\n",
    "print(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x ranges from (-50, 49), y = 20 * x + 5: we'll use gradient descent to fit parameters to this, \n",
    "# as if we had no idea y = 20*x + 5\n",
    "data = [(x, 0*(x**2) + 20*x + 5) for x in range(-10, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we think f(x) is a polynomial, and want to compute a gradient using coefficients ws\n",
    "def linear_gradient_mse(x: float, y: float, ws: Vector) ->  float:\n",
    "    predicted = sum([w * (x**i) for i, w in enumerate(ws)]) # our weights are coefficients to the polynomial\n",
    "    target = y \n",
    "    error = predicted - target  \n",
    "    grad = [2 * error * (x**i) for i in range(len(ws))]\n",
    "    return grad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.13139582251931237, 0.0022031918474523238]\n",
      "500 [86.23102134374972, 15.219123569003385]\n",
      "1000 [118.19749667312084, 15.710799151759428]\n",
      "1500 [130.0364143476073, 15.892893257751007]\n",
      "2000 [134.42100696406288, 15.960332570582063]\n",
      "2500 [136.04485915569532, 15.985309001808963]\n",
      "3000 [136.6462595983212, 15.99455912745]\n",
      "3500 [136.86899076452863, 15.997984950122493]\n",
      "4000 [136.95148018236725, 15.999253717860224]\n",
      "4500 [136.9820304828539, 15.999723611292032]\n",
      "4999 [136.99333168018492, 15.999897434734441]\n"
     ]
    }
   ],
   "source": [
    "weights_linear = [random.uniform(-1, 1) for i in range(2)]\n",
    "lr = 10**-3\n",
    "for epoch in range(5000):\n",
    "    # compute mean of gradients\n",
    "    mean_grad = vector_mean([linear_gradient_mse(x, y, weights_linear) for x, y in data])\n",
    "    weights_linear = gradient_step(weights_linear, mean_grad, -1 * lr)\n",
    "    if epoch % 500 == 0 or epoch == 4999:\n",
    "        print(epoch, weights_linear)  # second near 20, first near 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3491176762602788, 0.777818273803206, 0.30822641485799473]\n",
      "0 [-0.40855269949411116, 1.120490213696151, -3.508505750536359]\n",
      "1000 [0.6284423399413209, 20.044456344519602, -3.926866432310565]\n",
      "2000 [1.3407984513870967, 20.037248374177395, -3.9387828207464475]\n",
      "3000 [1.9370740542144136, 20.0311786630212, -3.948758305827073]\n",
      "4000 [2.4361851281666964, 20.02609802566323, -3.9571082618699127]\n",
      "5000 [2.8539648645185443, 20.021845290256813, -3.964097572699071]\n",
      "6000 [3.203666398335672, 20.01828554820825, -3.9699479586910393]\n",
      "7000 [3.4963832347766157, 20.015305874600223, -3.974845010359171]\n",
      "8000 [3.741401165927028, 20.01281174589952, -3.978944075800883]\n",
      "9000 [3.9464928419479857, 20.010724041407702, -3.982375188771322]\n",
      "9999 [4.1180074712879415, 20.00897813016922, -3.9852445693369587]\n",
      "10000 [4.118164341154576, 20.00897653333247, -3.9852471937156975]\n",
      "11000 [4.261861560913138, 20.00751378585791, -3.9876511986177747]\n",
      "12000 [4.382142976650603, 20.00628939656632, -3.989663465198489]\n",
      "13000 [4.482824249371936, 20.005264524424355, -3.9913478281498116]\n",
      "14000 [4.567099269038409, 20.00440665763756, -3.9927577201487052]\n",
      "15000 [4.637641473639304, 20.003688582285776, -3.993937866890228]\n",
      "16000 [4.696688658079565, 20.003087519021896, -3.9949257059109597]\n",
      "17000 [4.746113963257493, 20.002584400447645, -3.9957525742117803]\n",
      "18000 [4.787485297303104, 20.002163266242707, -3.9964447023546787]\n",
      "19000 [4.8221150739843335, 20.001810756858944, -3.997024046569128]\n",
      "20000 [4.8511018461215185, 20.001515689718403, -3.9975089852647425]\n",
      "21000 [4.875365154737934, 20.001268704470796, -3.9979149020455433]\n",
      "22000 [4.895674699458151, 20.001061966057218, -3.9982546737206555]\n",
      "23000 [4.912674755520725, 20.00088891616025, -3.998539078792506]\n",
      "24000 [4.926904611980411, 20.000744065155942, -3.9987771393808895]\n"
     ]
    }
   ],
   "source": [
    "data = [(x, -4*(x**2) + 20*x + 5) for x in range(-10, 10)]\n",
    "# now lets attempt to fit a cubic to a linear: the 2nd and 3rd degree terms (last 2) should near zero over time\n",
    "weights_cubic = [random.uniform(-1, 1) for i in range(3)]\n",
    "print(weights_cubic)\n",
    "lr = 2 * 10**-4 # slow down for visibility\n",
    "for epoch in range(25000):\n",
    "    # compute mean of gradients\n",
    "    mean_grad = vector_mean([linear_gradient_mse(x, y, weights_cubic) for x, y in data])\n",
    "    # print('mean_grad', mean_grad)\n",
    "    weights_cubic = gradient_step(weights_cubic, mean_grad, -1 * lr)\n",
    "    if epoch % 1000 == 0 or epoch == 9999:\n",
    "        print(epoch, weights_cubic)  # second near 20, first near 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9445286908931119, 0.20193238324695773, -0.05159987214657358, 0.25477570730816756]\n",
      "0 [-0.9506532034955649, 0.23097486963077318, -0.10585035870341508, 0.44523466729237776]\n",
      "1000 [-0.8955570292683016, 3.7929874249319586, -0.4977688367670486, 2.9828015403551373]\n",
      "2000 [-0.8032004393067043, 6.319507430906873, -0.7992220599917735, 2.5614234055428984]\n",
      "3000 [-0.6181605240651165, 8.410723638222807, -1.1075557655359858, 2.1989862987607056]\n",
      "4000 [-0.3649318214075337, 10.150708102621218, -1.3908745936785503, 1.8929546305811762]\n",
      "5000 [-0.06836195098100008, 11.603076834135482, -1.6492020082913355, 1.6340122989864045]\n",
      "6000 [0.25265234572006884, 12.819186851395765, -1.8840895351402445, 1.414224016339089]\n",
      "7000 [0.5841145165478869, 13.84071832840359, -2.0971943735696628, 1.2270784791195215]\n",
      "8000 [0.915831749119824, 14.701565335565274, -2.2901705880970322, 1.067231446233203]\n",
      "9000 [1.240540997891608, 15.429343791065415, -2.4646322610081817, 0.9302860180073766]\n",
      "9999 [1.552917261263811, 16.046032417903767, -2.6219804168670326, 0.812724231254065]\n",
      "10000 [1.553222712987175, 16.04660090977003, -2.62212993797011, 0.8126151433906823]\n",
      "11000 [1.850563553983439, 16.571786200166585, -2.764135315139113, 0.7112185327888736]\n",
      "12000 [2.130536849871005, 17.020031646800597, -2.892032048673772, 0.6236071916666595]\n",
      "13000 [2.392075990713164, 17.403779119867625, -3.007111119247385, 0.5477101242944927]\n",
      "14000 [2.6348210675056656, 17.733285409012524, -3.1105695401279645, 0.4817988452320628]\n",
      "15000 [2.858923157567909, 18.017029176995813, -3.203511473941507, 0.4244262064191477]\n",
      "16000 [3.0648938966388157, 18.262039258015196, -3.2869510402627347, 0.3743767431288839]\n",
      "17000 [3.253490556398414, 18.474159836299116, -3.361816266258764, 0.33062629778470204]\n",
      "18000 [3.4256288934024037, 18.658264934701393, -3.428953765584346, 0.2923091249163782]\n",
      "19000 [3.582317660738372, 18.818432162169582, -3.489133834397575, 0.2586910358048427]\n",
      "20000 [3.724609963374103, 18.958083686909234, -3.54305573392376, 0.22914742556969242]\n",
      "21000 [3.8535676608811733, 19.08010081813013, -3.591352991351719, 0.2031452528952015]\n",
      "22000 [3.9702358316905393, 19.186917313128475, -3.6345985988923823, 0.1802282246855441]\n",
      "23000 [4.075624954825713, 19.280595514041416, -3.6733100276431, 0.1600045838019581]\n",
      "24000 [4.1706989728304, 19.36288860879491, -3.707954000919936, 0.14213701493508807]\n",
      "25000 [4.256367800951075, 19.4352916627556, -3.738950992869673, 0.12633427741207007]\n",
      "26000 [4.333483164500043, 19.49908354883671, -3.7666794339729814, 0.11234424896650212]\n",
      "27000 [4.402836896172378, 19.555361488328177, -3.7914796166984597, 0.099948124911519]\n",
      "28000 [4.465161021804847, 19.605069581775904, -3.81365730300734, 0.08895556571014028]\n",
      "29000 [4.521129117683621, 19.649022442277165, -3.8334870413811046, 0.07920062499503287]\n",
      "30000 [4.571358543803214, 19.687924829369372, -3.85121520513051, 0.07053832154005599]\n",
      "31000 [4.616413252417224, 19.72238800971762, -3.8670627663994948, 0.06284174403722483]\n",
      "32000 [4.656806945336949, 19.75294343261407, -3.8812278218547482, 0.05599959799313654]\n",
      "33000 [4.693006411114489, 19.780054197168294, -3.8938878868290816, 0.04991412059156956]\n",
      "34000 [4.72543491797794, 19.804124698596674, -3.905201974875982, 0.04449930274880023]\n",
      "35000 [4.754475572925069, 19.82550876892049, -3.9153124794606535, 0.03967936843132483]\n",
      "36000 [4.780474583916703, 19.844516569211777, -3.9243468739845344, 0.035387470108799016]\n",
      "37000 [4.803744382370688, 19.86142044353267, -3.932419245613376, 0.031564566374442174]\n",
      "38000 [4.824566578510451, 19.87645990670298, -3.9396316775261155, 0.028158453598537133]\n",
      "39000 [4.8431947336488035, 19.889845907230058, -3.9460754932774327, 0.025122928244694028]\n",
      "40000 [4.859856942039698, 19.901764481745797, -3.951832376012348, 0.022417060376422125]\n",
      "41000 [4.874758221179755, 19.912379896977978, -3.956975374315091, 0.020004562079499074]\n",
      "42000 [4.888082713914979, 19.921837358737008, -3.961569805538669, 0.017853237155027703]\n",
      "43000 [4.899995708820633, 19.930265353894093, -3.9656740665598447, 0.01593450060625021]\n",
      "44000 [4.910645487398575, 19.937777680282615, -3.9693403610469873, 0.01422295823444528]\n",
      "45000 [4.920165007928911, 19.94447521039769, -3.9726153515205507, 0.01269603814564385]\n",
      "46000 [4.9286734365207305, 19.95044742732928, -3.975540743731761, 0.011333667205461606]\n",
      "47000 [4.936277536181625, 19.955773765230514, -3.978153810184694, 0.010117986510500552]\n",
      "48000 [4.943072924687062, 19.96052478155841, -3.9804878589804473, 0.009033100807120246]\n",
      "49000 [4.94914521176963, 19.964763184125367, -3.9825726535677712, 0.008064857512606308]\n"
     ]
    }
   ],
   "source": [
    "data = [(x, -4*(x**2) + 20*x + 5) for x in range(-3, 3)]\n",
    "# now lets attempt to fit a cubic to a linear: the 2nd and 3rd degree terms (last 2) should near zero over time\n",
    "weights_cubic = [random.uniform(-1, 1) for i in range(4)]\n",
    "print(weights_cubic)\n",
    "lr = 2 * 10**-4 # slow down for visibility\n",
    "for epoch in range(35000):\n",
    "    # compute mean of gradients\n",
    "    mean_grad = vector_mean([linear_gradient_mse(x, y, weights_cubic) for x, y in data])\n",
    "    # print('mean_grad', mean_grad)\n",
    "    weights_cubic = gradient_step(weights_cubic, mean_grad, -1 * lr)\n",
    "    if epoch % 1000 == 0 or epoch == 9999:\n",
    "        print(epoch, weights_cubic)  # second near 20, first near 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fin\n",
    "I spent way too much time just getting this to work and while the it can technically fit things correctly for arbitrary polynomials, the settings are extremely sensitive to both learning rate and batch size, causing major issues with overflow. Not worth digging too far into that, but I made changes to `vector_mean` that might help. Skipping **SGD** and **Batch-SGD** because they follow from these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsfsvenv",
   "language": "python",
   "name": "dsfsvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
